{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-encoders 101\n",
    "In this notebook you will implement a simple auto-encoder (AE). We assume that you are already familiar with the basics of neural networks. If not, please refer to *lab1*. We'll start by defining a simple AE similar to the one used for the finetuning step by [Geoffrey Hinton and Ruslan Salakhutdinov](https://www.cs.toronto.edu/~hinton/science.pdf). We'll experiment with the AE setup and try to run it on the MNIST dataset. There has been a wide variety of research into the field of auto-encoders and the technique that you're about to learn is very simple compared to recent advances (e.g. [the Ladder network](https://arxiv.org/abs/1507.02672) and [VAEs](https://arxiv.org/abs/1312.6114)). However, the basic idea stays the same.\n",
    "\n",
    "AEs are used within unsupervised learning, in which you do not have a target $y$. Instead it *encodes* an input $x$ into a latent state $z$ and decodes $z$ into a reconstruction $\\hat{x}$. This way the parameters of the network can be optimized w.r.t. the difference between $x$ and $\\hat{x}$. Depending on the input distribution, the difference can be measured in various ways, e.g. mean squared error (MSE). In many applications the auto-encoder will find a highly non-linear internal state of each data point corresponding to a strong feature. So if we are to model the MNIST dataset, one could expect that the internal state would correspond to a digit and/or the shape.\n",
    "\n",
    "*The exercises are found at the bottom of the notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "First let's load the MNIST dataset and plot a few examples. We only load a limited amount of number classes, so that we can speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# To speed up training we'll only work on a subset of the data containing only the numbers 0, 1.\n",
    "data = np.load('../lab1/mnist.npz')\n",
    "num_classes = 2\n",
    "idxs_train = []\n",
    "idxs_valid = []\n",
    "idxs_test = []\n",
    "for i in range(num_classes):\n",
    "    idxs_train += np.where(data['y_train'] == i)[0].tolist()\n",
    "    idxs_valid += np.where(data['y_valid'] == i)[0].tolist()\n",
    "    idxs_test += np.where(data['y_test'] == i)[0].tolist()\n",
    "\n",
    "x_train = data['X_train'][idxs_train].astype('float32')\n",
    "targets_train = data['y_train'][idxs_train].astype('int32') # Since this is unsupervised, the targets are only used for validation.\n",
    "x_train, targets_train = shuffle(x_train, targets_train, random_state=1234)\n",
    "\n",
    "\n",
    "x_valid = data['X_valid'][idxs_valid].astype('float32')\n",
    "targets_valid = data['y_valid'][idxs_valid].astype('int32')\n",
    "\n",
    "x_test = data['X_test'][idxs_test].astype('float32')\n",
    "targets_test = data['y_test'][idxs_test].astype('int32')\n",
    "\n",
    "print(\"training set dim(%i, %i).\" % x_train.shape)\n",
    "print(\"validation set dim(%i, %i).\" % x_valid.shape)\n",
    "print(\"test set dim(%i, %i).\" % x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot a few MNIST examples\n",
    "idx = 0\n",
    "canvas = np.zeros((28*10, 10*28))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_train[idx].reshape((28, 28))\n",
    "        idx += 1\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.axis('off')\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('MNIST handwritten digits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "When defining the model the latent layer must act as a bottleneck of information, so that we ensure that we find a strong internal representation. There are multiple ways of implementing an AE and you will soon experiment with the different variations. We initialize the AE with 1 hidden layer in the encoder and decoder using relu units as non-linearity. The latent layer has a dimensionality of 2. Since the input $x$ is pixel intensities that are normalized between 0 and 1, we use the properties of the sigmoid non-linearity to model the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, DenseLayer\n",
    "from lasagne.nonlinearities import rectify, sigmoid\n",
    "\n",
    "#define the model\n",
    "num_features = x_train.shape[1]\n",
    "\n",
    "l_in = InputLayer(shape=(None,num_features))\n",
    "l_enc = DenseLayer(incoming=l_in, num_units=128, nonlinearity=rectify)\n",
    "l_z = DenseLayer(incoming=l_enc, num_units=2, nonlinearity=None) # None indicates a linear output.\n",
    "l_dec = DenseLayer(l_z, num_units=128, nonlinearity=rectify)\n",
    "l_out = DenseLayer(l_dec, num_units=num_features, nonlinearity=sigmoid) # iid pixel intensities between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following we define the Theano functions for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.objectives import squared_error, binary_crossentropy\n",
    "\n",
    "sym_x = T.matrix('sym_x') # a symbolic variable taking on the value of a input batch.\n",
    "\n",
    "# Get network output\n",
    "train_out, train_z = lasagne.layers.get_output([l_out, l_z], sym_x, deterministic=False)\n",
    "eval_out, eval_z = lasagne.layers.get_output([l_out, l_z], sym_x, deterministic=True)\n",
    "\n",
    "\n",
    "# Get list of all trainable parameters in the network.\n",
    "all_params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "cost = squared_error(train_out, sym_x).mean()\n",
    "eval_cost = squared_error(train_out, sym_x).mean()\n",
    "\n",
    "# Let Theano do its magic and get all the gradients we need for training\n",
    "all_grads = T.grad(cost, all_params)\n",
    "\n",
    "\n",
    "# Set the update function for parameters \n",
    "# you might want to experiment with more advanced update schemes like adam, rmsprob, adadelta etc.\n",
    "updates = lasagne.updates.sgd(all_grads, all_params, learning_rate=0.25)\n",
    "\n",
    "\n",
    "f_eval = theano.function([sym_x],\n",
    "                     [eval_cost, eval_out, eval_z], on_unused_input='warn')\n",
    "\n",
    "f_train = theano.function([sym_x],\n",
    "                          [cost, train_out, train_z],\n",
    "                          updates=updates, on_unused_input='warn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop we sample each batch and evaluate the error, latent space and reconstructions every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "learning_rate = 0.1\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "updates = []\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "cur_loss = 0\n",
    "plt.figure(figsize=(12, 24))\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        #Forward->Backprob->Update params\n",
    "        cur_loss = []\n",
    "        for i in range(num_batches_train):\n",
    "            idxs = np.random.choice(range(x_train.shape[0]), size=(batch_size), replace=False)    \n",
    "            x_batch = x_train[idxs]\n",
    "            batch_loss, train_out, train_z = f_train(x_batch) #this will do the complete backprob pass\n",
    "            cur_loss += [batch_loss]\n",
    "        train_loss += [np.mean(cur_loss)]\n",
    "        updates += [batch_size*num_batches_train*(epoch+1)]\n",
    "\n",
    "        eval_loss, eval_out, eval_z = f_eval(x_valid)\n",
    "        valid_loss += [eval_loss]\n",
    "\n",
    "        if epoch == 0:\n",
    "            continue\n",
    "\n",
    "        # Plotting\n",
    "        plt.subplot(num_classes+1,2,1)\n",
    "        plt.title('Error')\n",
    "        plt.legend(['Train Error', 'Valid Error'])\n",
    "        plt.xlabel('Updates'), plt.ylabel('Error')\n",
    "        plt.plot(updates, train_loss, color=\"black\")\n",
    "        plt.plot(updates, valid_loss, color=\"grey\")\n",
    "        plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "        plt.grid('on')\n",
    "\n",
    "        plt.subplot(num_classes+1,2,2)\n",
    "        plt.cla()\n",
    "        plt.title('Latent space')\n",
    "        plt.xlabel('z0'), plt.ylabel('z1')\n",
    "        color = iter(plt.get_cmap('brg')(np.linspace(0, 1.0, num_classes)))\n",
    "        for i in range(num_classes):\n",
    "            clr = next(color)\n",
    "            plt.scatter(eval_z[targets_valid==i, 0], eval_z[targets_valid==i, 1], c=clr, s=5., lw=0, marker='o', )\n",
    "        plt.grid('on')\n",
    "        \n",
    "        c=0\n",
    "        for k in range(3, 3 + num_classes*2, 2):\n",
    "            plt.subplot(num_classes+1,2,k)\n",
    "            plt.cla()\n",
    "            plt.title('Inputs for %i' % c)\n",
    "            plt.axis('off')\n",
    "            idx = 0\n",
    "            canvas = np.zeros((28*10, 10*28))\n",
    "            for i in range(10):\n",
    "                for j in range(10):\n",
    "                    canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_valid[targets_valid==c][idx].reshape((28, 28))\n",
    "                    idx += 1\n",
    "            plt.imshow(canvas, cmap='gray')\n",
    "            \n",
    "            plt.subplot(num_classes+1,2,k+1)\n",
    "            plt.cla()\n",
    "            plt.title('Reconstructions for %i' % c)\n",
    "            plt.axis('off')\n",
    "            idx = 0\n",
    "            canvas = np.zeros((28*10, 10*28))\n",
    "            for i in range(10):\n",
    "                for j in range(10):\n",
    "                    canvas[i*28:(i+1)*28, j*28:(j+1)*28] = eval_out[targets_valid==c][idx].reshape((28, 28))\n",
    "                    idx += 1\n",
    "            plt.imshow(canvas, cmap='gray')\n",
    "            c+=1\n",
    "      \n",
    "        \n",
    "        plt.savefig(\"out.png\")\n",
    "        display(Image(filename=\"out.png\"))\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 1 - Analyzing the AE\n",
    "1. The above implementation of an AE is very simple.\n",
    "    - *Experiment with the number of layers and non-linearities in order to improve the reconstructions.*\n",
    "    - *What happens with the network when we change the non-linearities in the latent layer (e.g. sigmoid)?*\n",
    "    - *Try to increase the number of digit classes in the training set and analyze the results.*\n",
    "    - *Test different optimization algorithms and decide whether you should use regularizers*.\n",
    "       \n",
    "2. Currently we optimize w.r.t. mean squared error. \n",
    "    - *Find another error function that could fit this problem.* \n",
    "    - *Evaluate whether the error function is a better choice and explain your findings.*\n",
    "\n",
    "3. Complexity of the bottleneck.\n",
    "    - *Increase the number of units in the latent layer and train.*\n",
    "    - *Visualize by using [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) or [t-SNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2 - Adding classification (for the ambitious)\n",
    "The above training has been performed unsupervised. Now let us assume that we only have a fraction of labeled data points from each class (implemented below). As we know, semi-supervised learning can be utilized by combining unsupervised and supervised learning. Now you must analyze whether a trained AE from the above exercise can aid a classifier.\n",
    "\n",
    "1. Build a simple classifier (like the ones from lab1) where you:\n",
    "    - *Train on the labeled dataset and evaluate the results.*\n",
    "2. Build a second classifier and train on the latent output $z$ of the AE.\n",
    "3. Build a third classifier and train on the reconstructions of the AE.\n",
    "4. Evaluate the classifiers against each other and implement a model that improves the classification by combining the input, latent output and reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a subset of labeled data points\n",
    "\n",
    "num_labeled = 10 # You decide on the size of the fraction...\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes))\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out\n",
    "\n",
    "idxs_train_l = []\n",
    "for i in range(num_classes):\n",
    "    idxs = np.where(targets_train == i)[0]\n",
    "    idxs_train_l += np.random.choice(idxs, size=num_labeled).tolist()\n",
    "\n",
    "x_train_l = x_train[idxs_train_l]\n",
    "targets_train_l = targets_train[idxs_train_l]\n",
    "print(\"labeled training set dim(%i, %i).\" % x_train_l.shape)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "for i in range(num_classes*num_labeled):\n",
    "    im = x_train_l[i].reshape((28, 28))\n",
    "    plt.subplot(1, num_classes*num_labeled, i + 1)\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
